% !TEX root = altosaar-2020-thesis.tex
\chapter{Introduction}\label{ch:intro}
\lettrine[image=true,lines=3]{design/F}{rom} the development of novel antibiotics~\citep{stokes2020a-deep} to cataloging sources of light in the night sky~\citep{regier2019cataloging}, many domains in science can benefit from applied machine learning methods. However, the utility of such methods hinges on building the structure of a problem---knowledge about the data or task---into a machine learning solution. Whether the setting is statistical physics or recommender systems, an off-the-shelf machine learning method can serve as a starting point. But performance is sacrificed when a method cannot be customized to the specifics of an applied scientific problem. This thesis focuses on probabilistic modeling, where what is known about a problem can be molded into assumptions about a probability distribution. We develop probabilistic modeling methods that use the structure of a problem to yield meaningful solutions in the study of models with large numbers of random variables in statistical physics systems and recommender systems. In tandem, this thesis develops an algorithm to improve the accuracy of approximations to probabilistic models, through the use of the structure of a probability model during optimization. By both building scalable probabilistic modeling methods tailored to answer scientific questions, and improving flexible probabilistic modeling methods themselves, we highlight the reciprocal relationship between these aims.% suiting machine learning methods to a scientific domain and developing flexible probabilistic modeling techniques.

One example of an applied problem in statistical physics is the study of probable configurations of atoms in a material. Simulating a material to find likely configurations of atoms with statistical physics models can be expensive, but designing materials with improved properties is valuable~\citep{schmidt2019recent}. The computational cost of these simulations for studying statistical physics models can be reduced by doing math, for example in analytical calculations to develop approximations or to incorporate knowledge of how neighboring atoms interact into simulations~\citep{swendsen1987nonuniversal}. A challenge in studying statistical physics models is balancing problem-specific customization with the resulting computational savings. Machine learning techniques applied to statistical physics systems can be used to develop generic methods that exploit problem structure for better performance. These methods can be re-used across models, saving practitioners time.

Where statistical physics concerns probable configurations of interacting atoms, recommender systems find items a user is likely to interact with~\citep{koren2009matrix}. For example, humans eat. A meal recommender system can predict which meals someone is likely to consume. Such a recommendation model might inform its predictions using the history of meals a user has eaten, namely which foods comprise those meals. A property of this type of data is that items (meals) are associated with unordered collections of attributes (sets of foods). This means that the number of possible meals a user might consume is very large. Existing methods for this type of data either cannot scale to large numbers of datapoints, or fail to accurately predict which items a user is likely to consume. This highlights the need to imbue a recommendation model with both properties of the data (such as meals represented as unordered sets) and the goals of the recommendation problem, or accurate prediction of which items a user will consume.

% define variational approximation here?
Both statistical physics models and recommender systems can be framed as probability models, the former as probable configurations of atoms, and the latter as probable items users may consume. Probabilistic modeling relies on inferring the parameters of a probability model using knowledge about the structure of the problem. For example, knowledge in the form of data regarding which meals someone has eaten can inform the predictions of a recommendation model; or, knowledge of how neighboring atoms interact in a material can be used in a probabilistic model of that material.

Practitioners that work with probability models seek probabilistic inferences. For example, the goals of such inferences include computing probabilities, summing over the random variables in a probability model, or finding likely configurations of random variables. Common inference methods are Markov Chain Monte Carlo~\citep{metropolis1953equation}, variational inference~\citep{blei2017variational}, and maximum likelihood estimation~\citep{bishop2006pattern}. This thesis uses variational inference and maximum likelihood estimation, as both algorithms can be scaled to large probability models~\citep{hoffman2013stochastic,robbins1951a-stochastic}. In particular, the variational inference algorithm can aid probabilistic inference in interacting systems of random variables found in statistical physics models. However, variational inference is sensitive to the initial choice of parameters governing the probability of the random variables under study. The accuracy of inferences of likely configurations of random variables can suffer, depending on this choice of initial parameters. Inference algorithms such as variational inference are utile in applied domains insofar as their performance is independent of the initial choice of parameters.

This thesis is organized as follows. \Cref{ch:background} introduces probabilistic models and gives examples of their use in statistical physics and recommender systems. We also review two approaches for statistical inference in probability models: variational inference and maximum likelihood estimation. \Cref{ch:hvm} develops and applies variational inference methods for statistical physics models. We show that exploiting the structure of a statistical physics model in a variational inference method is advantageous. This work was presented in \citet{altosaar2019hierarchical}. \Cref{ch:rfs} develops probability models for recommending items with sets of attributes and is based on \citet{altosaar2020rankfromsets:}. Similar to variational methods in physics, accounting for the structure of the problem helps: probability models that represent set-valued datapoints and the goals of the recommendation task are accurate and scalable. \Cref{ch:pvi} develops \gls{PVI} based on \citet{altosaar2018proximity}. \gls{PVI} is an inference algorithm that is imbued with information about a probability distribution we wish to infer. In this case, the structure of the problem is information about a probability distribution, which is used to inform an algorithm for fitting a probability model. We show how this enables \gls{PVI} to obtain accurate solutions. Finally, \Cref{ch:discussion} reviews how knowledge about a problem is useful in building probabilistic models for science solutions. We apply the \gls{PVI} algorithm developed in \Cref{ch:pvi} to the statistical physics setting of \Cref{ch:hvm} and the recommender systems application of \Cref{ch:rfs}. This highlights that methods development goes hand-in-hand with the aims of applied probabilistic modeling. We close with a discussion of extensions of this line of work.

