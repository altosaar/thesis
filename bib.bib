
%% Created for Jaan Altosaar at 2020-05-19 11:25:03 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{huang2019clinicalbert:,
	Author = {Kexin Huang and Jaan Altosaar and Rajesh Ranganath},
	Date-Added = {2020-05-19 11:21:10 -0400},
	Date-Modified = {2020-05-19 11:22:26 -0400},
	Journal = {ACM Conference on Health, Inference, and Learning},
	Title = {ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},
	Year = {2020}}

@article{rainforth2018tighter,
	Author = {Tom Rainforth and Adam R. Kosiorek and Tuan Anh Le and Chris J. Maddison and Maximilian Igl and Frank Wood and Yee Whye Teh},
	Date-Added = {2020-05-19 11:11:07 -0400},
	Date-Modified = {2020-05-19 11:11:22 -0400},
	Journal = {International Conference on Machine Learning},
	Title = {Tighter Variational Bounds are Not Necessarily Better},
	Year = {2018}}

@article{tucker2018doubly,
	Author = {George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},
	Date-Added = {2020-05-19 11:09:47 -0400},
	Date-Modified = {2020-05-19 11:10:04 -0400},
	Journal = {International Conference on Learning Representations},
	Title = {Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},
	Year = {2019}}

@inproceedings{devlin2019bert:,
	Abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	Address = {Minneapolis, Minnesota},
	Author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	Booktitle = {Association for Computational Linguistics},
	Date-Added = {2020-05-09 13:37:37 -0400},
	Date-Modified = {2020-05-19 11:24:21 -0400},
	Doi = {10.18653/v1/N19-1423},
	Month = jun,
	Pages = {4171--4186},
	Publisher = {Association for Computational Linguistics},
	Title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	Url = {https://www.aclweb.org/anthology/N19-1423},
	Year = {2019}}

@inproceedings{vaswani2017attention,
	Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	Booktitle = {Neural Information Processing Systems},
	Date-Added = {2020-05-09 13:36:20 -0400},
	Date-Modified = {2020-05-09 13:36:58 -0400},
	Title = {Attention is All you Need},
	Year = {2017}}

@article{dziugaite2017computing,
	Author = {Gintare Karolina Dziugaite and Daniel M. Roy},
	Date-Added = {2020-05-09 12:59:50 -0400},
	Date-Modified = {2020-05-09 13:00:06 -0400},
	Journal = {Uncertainty in Artificial Intelligence},
	Title = {Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data},
	Year = {2017}}

@article{bloem-reddy2019probabilistic,
	Archiveprefix = {arXiv},
	Author = {Benjamin Bloem-Reddy and Yee Whye Teh},
	Date-Added = {2020-05-09 12:49:37 -0400},
	Date-Modified = {2020-05-19 11:25:03 -0400},
	Eprint = {1901.06082},
	Journal = {arXiv:1901.06082},
	Primaryclass = {stat.ML},
	Title = {Probabilistic symmetry and invariant neural networks},
	Year = {2019}}

@article{wang2019frequentist,
	Author = {Yixin Wang and David M. Blei},
	Date-Added = {2020-04-30 16:15:13 -0400},
	Date-Modified = {2020-04-30 16:15:14 -0400},
	Doi = {10.1080/01621459.2018.1473776},
	Eprint = {https://doi.org/10.1080/01621459.2018.1473776},
	Journal = {Journal of the American Statistical Association},
	Number = {527},
	Pages = {1147-1161},
	Publisher = {Taylor & Francis},
	Title = {Frequentist Consistency of Variational Bayes},
	Url = {https://doi.org/10.1080/01621459.2018.1473776},
	Volume = {114},
	Year = {2019}}

@book{stone2013the-theory,
	Address = {Oxford},
	Author = {Stone, Anthony J},
	Date-Added = {2020-04-30 09:35:47 -0400},
	Date-Modified = {2020-04-30 09:35:48 -0400},
	Doi = {10.1093/acprof:oso/9780199672394.001.0001},
	Publisher = {Oxford University Press},
	Title = {{The theory of intermolecular forces; 2nd ed.}},
	Url = {https://cds.cern.ch/record/1520111},
	Year = {2013}}

@article{swendsen1987nonuniversal,
	Author = {Swendsen, Robert H. and Wang, Jian-Sheng},
	Date-Added = {2020-04-16 18:10:09 -0400},
	Date-Modified = {2020-04-17 10:40:26 -0400},
	Doi = {10.1103/PhysRevLett.58.86},
	Issue = {2},
	Journal = {Physical Review Letters},
	Month = {Jan},
	Numpages = {0},
	Pages = {86--88},
	Publisher = {American Physical Society},
	Title = {Nonuniversal critical dynamics in Monte Carlo simulations},
	Url = {https://link.aps.org/doi/10.1103/PhysRevLett.58.86},
	Volume = {58},
	Year = {1987}}

@article{lecun2015deep,
	Abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	Author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	Da = {2015/05/01},
	Date-Added = {2020-04-16 10:43:41 -0400},
	Date-Modified = {2020-04-16 10:43:41 -0400},
	Doi = {10.1038/nature14539},
	Id = {LeCun2015},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {7553},
	Pages = {436--444},
	Title = {Deep learning},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature14539},
	Volume = {521},
	Year = {2015}}

@article{henelius2016refrustration,
	Author = {Henelius, P. and Lin, T. and Enjalran, M. and Hao, Z. and Rau, J. G. and Altosaar, J. and Flicker, F. and Yavors'kii, T. and Gingras, M. J. P.},
	Date-Added = {2020-04-15 21:29:39 -0400},
	Date-Modified = {2020-04-15 21:30:00 -0400},
	Doi = {10.1103/PhysRevB.93.024402},
	Issue = {2},
	Journal = {Physical Review B},
	Month = {Jan},
	Numpages = {23},
	Pages = {024402},
	Publisher = {American Physical Society},
	Title = {Refrustration and competing orders in the prototypical ${\mathrm{Dy}}_{2}{\mathrm{Ti}}_{2}{\mathrm{O}}_{7}$ spin ice material},
	Url = {https://link.aps.org/doi/10.1103/PhysRevB.93.024402},
	Volume = {93},
	Year = {2016}}

@article{schmidt2019recent,
	Abstract = {One of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of statistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At present, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide a comprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning principles, algorithms, descriptors, and databases in materials science. We continue with the description of different machine learning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research in numerous quantitative structure--property relationships and various approaches for the replacement of first-principle methods by machine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design process and related examples of applications. Two major questions are always the interpretability of and the physical understanding gained from machine learning models. We consider therefore the different facets of interpretability and their importance in materials science. Finally, we propose solutions and future research paths for various challenges in computational materials science.},
	Author = {Schmidt, Jonathan and Marques, M{\'a}rio R. G. and Botti, Silvana and Marques, Miguel A. L.},
	Da = {2019/08/08},
	Date-Added = {2020-04-15 10:00:35 -0400},
	Date-Modified = {2020-04-15 10:00:36 -0400},
	Doi = {10.1038/s41524-019-0221-0},
	Id = {Schmidt2019},
	Isbn = {2057-3960},
	Journal = {npj Computational Materials},
	Number = {1},
	Pages = {83},
	Title = {Recent advances and applications of machine learning in solid-state materials science},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41524-019-0221-0},
	Volume = {5},
	Year = {2019}}

@article{shamay2018quantitative,
	Abstract = {Development of targeted nanoparticle drug carriers often requires complex synthetic schemes involving both supramolecular self-assembly and chemical modification. These processes are generally difficult to predict, execute, and control. We describe herein a targeted drug delivery system that is accurately and quantitatively predicted to self-assemble into nanoparticles based on the molecular structures of precursor molecules, which are the drugs themselves. The drugs assemble with the aid of sulfated indocyanines into particles with ultrahigh drug loadings of up to 90{\%}. We devised quantitative structure-nanoparticle assembly prediction (QSNAP) models to identify and validate electrotopological molecular descriptors as highly predictive indicators of nano-assembly and nanoparticle size. The resulting nanoparticles selectively targeted kinase inhibitors to caveolin-1-expressing human colon cancer and autochthonous liver cancer models to yield striking therapeutic effects while avoiding pERK inhibition in healthy skin. This finding enables the computational design of nanomedicines based on quantitative models for drug payload selection.},
	Author = {Shamay, Yosi and Shah, Janki and I{\c s}ık, Mehtap and Mizrachi, Aviram and Leibold, Josef and Tschaharganeh, Darjus F. and Roxbury, Daniel and Budhathoki-Uprety, Januka and Nawaly, Karla and Sugarman, James L. and Baut, Emily and Neiman, Michelle R. and Dacek, Megan and Ganesh, Kripa S. and Johnson, Darren C. and Sridharan, Ramya and Chu, Karen L. and Rajasekhar, Vinagolu K. and Lowe, Scott W. and Chodera, John D. and Heller, Daniel A.},
	Da = {2018/04/01},
	Date-Added = {2020-04-15 09:59:23 -0400},
	Date-Modified = {2020-04-15 09:59:24 -0400},
	Doi = {10.1038/s41563-017-0007-z},
	Id = {Shamay2018},
	Isbn = {1476-4660},
	Journal = {Nature Materials},
	Number = {4},
	Pages = {361--368},
	Title = {Quantitative self-assembly prediction yields targeted nanomedicines},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41563-017-0007-z},
	Volume = {17},
	Year = {2018}}

@article{huan2017a-universal,
	Abstract = {Emerging machine learning (ML)-based approaches provide powerful and novel tools to study a variety of physical and chemical problems. In this contribution, we outline a universal strategy to create ML-based atomistic force fields, which can be used to perform high-fidelity molecular dynamics simulations. This scheme involves (1) preparing a big reference dataset of atomic environments and forces with sufficiently low noise, e.g., using density functional theory or higher-level methods, (2) utilizing a generalizable class of structural fingerprints for representing atomic environments, (3) optimally selecting diverse and non-redundant training datasets from the reference data, and (4) proposing various learning approaches to predict atomic forces directly (and rapidly) from atomic configurations. From the atomistic forces, accurate potential energies can then be obtained by appropriate integration along a reaction coordinate or along a molecular dynamics trajectory. Based on this strategy, we have created model ML force fields for six elemental bulk solids, including Al, Cu, Ti, W, Si, and C, and show that all of them can reach chemical accuracy. The proposed procedure is general and universal, in that it can potentially be used to generate ML force fields for any material using the same unified workflow with little human intervention. Moreover, the force fields can be systematically improved by adding new training data progressively to represent atomic environments not encountered previously.},
	Author = {Huan, Tran Doan and Batra, Rohit and Chapman, James and Krishnan, Sridevi and Chen, Lihua and Ramprasad, Rampi},
	Da = {2017/09/18},
	Date-Added = {2020-04-15 09:54:28 -0400},
	Date-Modified = {2020-04-15 09:54:30 -0400},
	Doi = {10.1038/s41524-017-0042-y},
	Id = {Huan2017},
	Isbn = {2057-3960},
	Journal = {npj Computational Materials},
	Number = {1},
	Pages = {37},
	Title = {A universal strategy for the creation of machine learning-based atomistic force fields},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41524-017-0042-y},
	Volume = {3},
	Year = {2017}}

@inproceedings{altosaar2018proximity,
	Author = {Altosaar, Jaan and Ranganath, Rajesh and Blei, David M.},
	Booktitle = {Artificial Intelligence and Statistics},
	Date-Added = {2020-04-15 08:19:50 -0400},
	Date-Modified = {2020-04-15 08:19:59 -0400},
	Title = {Proximity Variational Inference},
	Year = {2018}}

@article{altosaar2020rankfromsets:,
	Author = {Altosaar, Jaan and Tansey, Wesley and Ranganath, Rajesh},
	Date-Added = {2020-04-15 08:18:28 -0400},
	Date-Modified = {2020-04-15 08:19:34 -0400},
	Journal = {American Statistical Association Symposium on Data Science \& Statistics},
	Title = {{RankFromSets}: Scalable Set Recommendation with Optimal Recall},
	Year = {2020}}

@inproceedings{altosaar2019hierarchical,
	Author = {Altosaar, Jaan and Ranganath, Rajesh and Cranmer, Kyle},
	Booktitle = {Machine Learning and the Physical Sciences Workshop, Neural Information Processing Systems},
	Date-Added = {2020-04-15 08:17:26 -0400},
	Date-Modified = {2020-04-17 11:02:57 -0400},
	Title = {Hierarchical Variational Models for Statistical Physics},
	Year = {2019}}

@article{regier2019cataloging,
	Abstract = {A key task in astronomy is to locate astronomical objects in images and to characterize them according to physical parameters such as brightness, color, and morphology. This task, known as cataloging, is challenging for several reasons: many astronomical objects are much dimmer than the sky background, labeled data is generally unavailable, overlapping astronomical objects must be resolved collectively, and the datasets are enormous -- terabytes now, petabytes soon. In this work, we infer an astronomical catalog from 55 TB of imaging data using Celeste, a Bayesian variational inference code written entirely in the high-productivity programming language Julia. Using over 1.3 million threads on 650,000 Intel Xeon Phi cores of the Cori Phase II supercomputer, Celeste achieves a peak rate of 1.54 DP PFLOP/s. Celeste is able to jointly optimize parameters for 188 M stars and galaxies, loading and processing 178 TB across 8192 nodes in 14.6 min. To achieve this, Celeste exploits parallelism at multiple levels (cluster, node, and thread) and accelerates I/O through Cori's burst buffer. Julia's native performance enables Celeste to employ high-level constructs without resorting to hand-written or generated low-level code (C/C++/Fortran) while still achieving petascale performance.},
	Author = {Jeffrey Regier and Keno Fischer and Kiran Pamnany and Andreas Noack and Jarrett Revels and Maximilian Lam and Steve Howard and Ryan Giordano and David Schlegel and Jon McAuliffe and Rollin Thomas and Prabhat},
	Date-Added = {2020-04-14 14:27:19 -0400},
	Date-Modified = {2020-04-14 14:27:19 -0400},
	Doi = {https://doi.org/10.1016/j.jpdc.2018.12.008},
	Issn = {0743-7315},
	Journal = {Journal of Parallel and Distributed Computing},
	Keywords = {Astronomy, Bayesian, Distributed optimization, Variational inference, Julia, High-performance computing},
	Pages = {89 - 104},
	Title = {Cataloging the visible universe through Bayesian inference in Julia at petascale},
	Url = {http://www.sciencedirect.com/science/article/pii/S0743731518304672},
	Volume = {127},
	Year = {2019}}

@article{stokes2020a-deep,
	Abstract = {Summary
Due to the rapid emergence of antibiotic-resistant bacteria, there is a growing need to discover new antibiotics. To address this challenge, we trained a deep neural network capable of predicting molecules with antibacterial activity. We performed predictions on multiple chemical libraries and discovered a molecule from the Drug Repurposing Hub---halicin---that is structurally divergent from conventional antibiotics and displays bactericidal activity against a wide phylogenetic spectrum of pathogens including Mycobacterium tuberculosis and carbapenem-resistant Enterobacteriaceae. Halicin also effectively treated Clostridioides difficile and pan-resistant Acinetobacter baumannii infections in murine models. Additionally, from a discrete set of 23 empirically tested predictions from >107 million molecules curated from the ZINC15 database, our model identified eight antibacterial compounds that are structurally distant from known antibiotics. This work highlights the utility of deep learning approaches to expand our antibiotic arsenal through the discovery of structurally distinct antibacterial molecules.},
	Author = {Jonathan M. Stokes and Kevin Yang and Kyle Swanson and Wengong Jin and Andres Cubillos-Ruiz and Nina M. Donghia and Craig R. MacNair and Shawn French and Lindsey A. Carfrae and Zohar Bloom-Ackerman and Victoria M. Tran and Anush Chiappino-Pepe and Ahmed H. Badran and Ian W. Andrews and Emma J. Chory and George M. Church and Eric D. Brown and Tommi S. Jaakkola and Regina Barzilay and James J. Collins},
	Date-Added = {2020-04-14 14:23:13 -0400},
	Date-Modified = {2020-04-14 14:23:13 -0400},
	Doi = {https://doi.org/10.1016/j.cell.2020.01.021},
	Issn = {0092-8674},
	Journal = {Cell},
	Keywords = {antibiotics, antibiotic resistance, antibiotic tolerance, machine learning, drug discovery},
	Number = {4},
	Pages = {688 - 702.e13},
	Title = {A Deep Learning Approach to Antibiotic Discovery},
	Url = {http://www.sciencedirect.com/science/article/pii/S0092867420301021},
	Volume = {180},
	Year = {2020}}
