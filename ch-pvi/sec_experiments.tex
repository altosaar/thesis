% !TEX root = ../main.tex
\section{Empirical Study}
\label{sec:case-study}
\label{sec:pvi-experiments}
\input{ch-pvi/tab_sbn}
We developed \acrfull{PVI}. We now empirically study \gls{PVI}, variational inference, and deterministic annealing~\citep{Katihara:2008}.\footnote{Source code for reproducibility is available at \url{https://github.com/altosaar/proximity_vi}.}

We first study sigmoid belief networks and find that \gls{PVI} improves over deterministic annealing and \gls{VI} in terms of held-out values of the \gls{ELBO} and marginal likelihood. We then study a variational autoencoder model of images.  Using an orthogonal proximity statistic, we show that \gls{PVI} improves over classical \gls{VI} by reducing overpruning. Finally, we study a deep generative model fit to a large corpus of text, where \gls{PVI} yields better predictive performance with little hyperparameter tuning.\footnote{We also compared \gls{PVI} to \citet{khan2015kullback}. Specifically, we tested \gls{PVI} on the Bayesian logistic regression model from that paper and with the same data.  Because Bayesian logistic regression has a single mode, all   methods performed equally well.  We note that we could not apply their algorithm to the sigmoid belief network because it would require approximating difficult iterated expectations.}

\paragraph{Hyperparameters.} For \gls{PVI}, we use the inverse Huber distance for $d$.\footnote{We define the inverse Huber distance   $d(x, y)$ to be $|x - y|$ if $|x - y| < 1$ and $0.5(x-y)^2 + 0.5$   otherwise. The constants ensure the function and its derivative are   continuous at $|x-y| = 1$.} The inverse Huber distance penalizes smaller values than the square difference. For \gls{PVI} \Cref{algo:global}, we set the exponential moving average decay constant for $\tmblambda$ to $\alpha=0.9999$. We set the constraint scale $k$ (or temperature parameter in deterministic annealing) to the initial absolute value of the \gls{ELBO} unless otherwise specified. We explore two annealing schedules for \gls{PVI} and deterministic annealing: a linear decay and an exponential decay. For the exponential decay, the value of the magnitude at iteration $t$ of $T$ total iterations is set to $k\cdot \gamma^{\frac{t}{T}}$ where $\gamma$ is the decay rate. We use the Adam optimizer~\citep{kingma2015adam:} unless otherwise specified.
\subsection{Sigmoid Belief Network}
The sigmoid belief network is a discrete latent variable model with layers of Bernoulli latent variables~\citep{neal1992connectionist,ranganath2015deep}. It is used to benchmark variational inference algorithms~\citep{Mnih:2016:VIM:3045390.3045621}. The approximate posterior is a collection of Bernoullis, parameterized by an inference network with weights and biases.  We fit these variational parameters with \gls{VI}, deterministic annealing~\citep{Katihara:2008}, or \gls{PVI}, and learn the model parameters (weights and biases) using variational expectation-maximization.

\input{ch-pvi/tab_sbn_3}
We learn the weights and biases of the model with gradient ascent. We use a step size of $\rho=10^{-3}$ and train for $4\times10^6$ iterations with a batch size of $20$.  For \gls{PVI} \Cref{algo:global} and deterministic annealing, we grid search over exponential decays with rates $\gamma\in \{10^{-5}, 10^{-6}, ..., 10^{-10}, 10^{-20}, 10^{-30}\}$ and report the best results for each algorithm.  (We also explored linear decays but they did not perform as well.)  To reduce the variance of the gradients, we use the leave-one-out control variate of~\citet{Mnih:2016:VIM:3045390.3045621} with $5$ samples. (This is an extension to the black box variational inference algorithm in \citet{ranganath2014black}.)

\paragraph{Results on MNIST.} We train a sigmoid belief network model on the binary MNIST dataset of handwritten digits~\citep{pmlr-v15-larochelle11a}. For evaluation, we compute the \gls{ELBO} and held-out marginal likelihood with importance sampling on the validation set of $10^4$ digits using $5000$ samples, as in \citet{rezende2014stochastic}. In Table~$1$ we show the results for a model with one layer of $200$ latent variables. \Cref{table:sbn_3_layer} displays similar results for a three-layer model with $200$~latent variables per layer. In both one and three-layer models the \gls{KL} proximity statistic performs worse than the mean/variance and entropy statistics; it requires different decay schedules. Overall, \gls{PVI} with the entropy and mean/variance proximity statistics yields improvements in the held-out marginal likelihood in comparison to deterministic annealing and \gls{VI}.

\input{ch-pvi/tab_dlgm}
\subsection{Variational Autoencoder}
\label{sec:variational_autoencoder}
To demonstrate the value of designing proximity statistics tailored to specific   models,  we study the variational autoencoder~ \citep{kingma2014autoencoding,rezende2014stochastic}. This model is difficult to optimize, and current optimization techniques yield solutions that do not use the full model capacity~\citep{Burda2016}. In \Cref{sec:proximity_examples} we designed an orthogonal proximity statistic to make backpropagation in neural networks easier. We show that this statistic enables us to find a better approximate posterior in the variational autoencoder by reducing overpruning.

We fit the variational autoencoder to binary MNIST data~\citep{pmlr-v15-larochelle11a} with variational expectation-maximization. The model has one layer of $100$ Gaussian latent variables. The inference network and generative network are chosen to have two hidden layers of size $200$ with rectified linear units. We use an orthogonal initialization for the inference network weights. The learning rate is set to $10^{-3}$ and we run \gls{VI} and \gls{PVI} for $5\times 10^4$ iterations. The orthogonal proximity statistic changes rapidly during optimization, so we use constraint magnitudes $k \in \{1, 10^{-1}, 10^{-2}, ..., 10^{-5}\}$, with no decay, and report the best result.

We compute the \gls{ELBO} and importance-sampled marginal likelihood estimates on the validation set. \Cref{table:dlgm2} shows that \gls{PVI} with the orthogonal proximity statistic on the weights of the inference network enables easier optimization and improves over \gls{VI}.

Why does \gls{PVI} improve upon \gls{VI} in the variational autoencoder? The choice of rectified linear units in the inference network allows us to  study overpruning of the latent code~\citep{mackay2001,Burda2016}. We study  the fraction of `dead units'--- the fraction of rectified linear units in each layer of the inference   neural network whose input is below zero. With \gls{PVI} \Cref{algo:global} and the orthogonal proximity constraint, the inference network has $1.6\%$ fewer dead units in the hidden layer and shows a $3.2\%$ reduction in the output layer than in the same model learned using classic variational inference.

Once the input to a rectified linear unit drops below zero, the unit stops receiving gradient updates. The output layer parametrizes the latent variable distribution, so this means \gls{PVI} reduced the pruning of the approximate posterior and led to the utilization of $3$ additional latent variables. This is the reason it outperformed a variational autoencoder fit with \gls{VI}.

\input{ch-pvi/tab_poisson}
\subsection{Deep Generative Model of Text}
\label{sec:poisson_factor_analysis}
Deep exponential family models, Bayesian analogues to neural networks, represent a flexible class of models~\citep{ranganath2015deep}. However, black box variational inference is commonly used to fit these models, which requires variance reduction~\citep{ranganath2014black}. Deep exponential family models with Poisson latent variables present a challenging approximate inference problem because they are discrete and high-variance. We demonstrate that \gls{PVI} with the mean/variance proximity constraint improves predictive performance in such an unsupervised model of text.

The generative process for a single-layer deep exponential family model of text, with Poisson latent variables and Poisson likelihood, is
\begin{align*}
\mbz &\sim \textrm{Poisson}(\mblambda) \\
\mbx &\sim \textrm{Poisson}(\mbz^\top g(W)) \,,
\end{align*}
where $W$ are real-valued model parameters and $g$ is an elementwise function that maps to the positive reals (we use the softplus function). The dimension of $\mbz$ is $K$, so the model parameters must have shape $(K, V)$ where $V$ is the cardinality of the count-valued observations $\mbx$. We use this as a model of documents, so $\mbx$ is the bag-of-words representation of word counts, $W$ represents the common factors in documents, and the per-document latent variable $\mbz$ captures factors prevalent in documents' language.

We study the performance of our method on a corpus of articles from the academic journal \emph{Science}. The corpus contains $138$k documents in the training set, $1$k documents in the test set, and $5.9$k terms. We set the latent dimension to $100$, and fit the variational Poisson parameters using black box variational inference~\citep{ranganath2014black} using minibatches of size $64$ and $32$~samples of the latent variables to estimate the gradients.

Poisson variables have high variance, so we use  the optimal control variate scaling developed in \citet{ranganath2014black} and estimate this scaling in a round-robin fashion as in \citet{Mnih:2016:VIM:3045390.3045621} for efficiency. We use the RMSProp adaptive gradient optimizer~\citep{Hinton} with a step size of $0.01$. For \gls{PVI}~\Cref{algo:global} with the mean/variance proximity statistic, we use an exponential decay for the constraint and test decay rates $\gamma$ of $10^{-5}$ and $10^{-10}$. We train for $10^6$ iterations on the \emph{Science} corpus, using variational expectation-maximization to learn the model parameters.

\input{ch-pvi/tab_poisson_topics}
For evaluation, we keep the model parameters fixed and hold out $90\%$ of the words in each document in the test set. Using the $10\%$ of observed words in each document, we learn the variational parameters using \gls{PVI} or variational inference with $300$ iterations per document. We compute perplexity on the held-out documents, which is given by
\begin{align*}
\exp\left(\frac{-\sum_{d\in{\textrm{docs}}} \sum_{w\in d} \log p(w \mid
\textrm{\# held-out in d})}{N_{\textrm{held-out words}}}\right).
\end{align*}
Conditional on the number of held-out words in a document, the distribution over held-out words is multinomial. The mean of the conditional multinomial is the normalized Poisson rate of the document matrix-multiplied with the softplus of the weights. This is the same evaluation metric as in \citet{ranganath2015deep}. The results of fitting the model to the corpus of \emph{Science} documents are reported in \Cref{table:poisson} and \Cref{table:poisson_topics}. While the topics found by models fit with both \gls{PVI} and \gls{VI} are similar, \gls{PVI} gives better predictive performance in terms of held-out perplexity.