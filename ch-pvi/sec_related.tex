% !TEX root = ../main.tex
\parhead{Related Work.}  Recent work has proposed several related algorithms. \citet{khan2015kullback} and \citet{Theis2015} develop a method to optimize the \gls{ELBO} that imposes a soft limit on the change in \gls{KL} of consecutive variational approximations. This is equivalent to \gls{PVI} with identity proximity statistics and a \gls{KL} distance function. \citet{Khan:2016:FSV:3020948.3020982} extend both prior works to other divergence functions. Their general approach is equivalent to \gls{PVI} identity proximity statistics and distance functions given by strongly-convex divergences. Compared to prior work, \gls{PVI} generalizes to a broader class of proximity statistics. We develop proximity statistics based on entropy, \gls{KL}, orthogonal weight matrices, and the mean and variance of the variational approximation.

The problem of model pruning in variational inference has also been studied and
analytically solved in a matrix factorization model in \citet{nakajima2013}---this method is model-specific, whereas \gls{PVI} applies to a much broader class of latent variable models. Finally, deterministic annealing~\citep{Katihara:2008} consists of adding a temperature parameter to the entropy term in the \gls{ELBO} that initialized to a large value then annealed to unity during inference. This is similar to \gls{PVI} with the entropy proximity statistic which keeps the entropy stable across iterations. Deterministic annealing enforces global penalization of low-entropy configurations of latent variables rather than the smooth constraint used in \gls{PVI}, and cannot accommodate the range of proximity statistics we design in this work.